# -*- coding: utf-8 -*-
"""vehical_price.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FM5NN4214TD8x5u_zmSWoHO5VtVqfHAY
"""

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/dataset.csv')
print("Data loaded successfully.")

print(f"Initial shape: {df.shape}")
print("\nInitial Data Types and Missing Values:")
df.info()

def clean_and_convert_mileage(mileage_series):
    """Cleans mileage column by removing non-numeric characters and converting to float."""
    # Convert to string first to handle potential mixed types, then remove all non-digit/non-decimal chars
    cleaned_mileage = mileage_series.astype(str).str.replace(r'[^\d.]', '', regex=True).replace('', np.nan)
    return pd.to_numeric(cleaned_mileage, errors='coerce')

def clean_and_convert_numerical_column(series):
    """Cleans numerical columns (like cylinders, doors) which might have string artifacts."""
    # Convert to string, remove non-numeric chars, and convert to integer/float
    cleaned_series = series.astype(str).str.replace(r'[^\d.]', '', regex=True).replace('', np.nan)
    return pd.to_numeric(cleaned_series, errors='coerce')

def clean_price(price_series):
    """Removes non-numeric characters from price and ensures numeric type."""
    cleaned_price = price_series.astype(str).str.replace(r'[$,]', '', regex=True).replace('', np.nan)
    return pd.to_numeric(cleaned_price, errors='coerce')

# Apply cleaning
df['price'] = clean_price(df['price'])
df['mileage'] = clean_and_convert_mileage(df['mileage'])
df['cylinders'] = clean_and_convert_numerical_column(df['cylinders'])
df['doors'] = clean_and_convert_numerical_column(df['doors'])

df = df.dropna(subset=['price'])
df = df[df['price'] > 0]
print(f"\nShape after dropping missing/zero price: {df.shape}")

current_year = pd.to_datetime('now').year
df['car_age'] = current_year - df['year']
df.drop(columns=['year'], inplace=True) # Year is now represented by age

plt.figure(figsize=(10, 6))
sns.histplot(df['price'], bins=50, kde=True)
plt.title('Distribution of Vehicle Price (Before Outlier Removal)')
plt.xlabel('Price (USD)')
plt.show()

price_99th_percentile = df['price'].quantile(0.99)
df['price'] = np.where(df['price'] > price_99th_percentile, price_99th_percentile, df['price'])
print(f"Price capped at 99th percentile: ${price_99th_percentile:,.2f}")

mileage_99th_percentile = df['mileage'].quantile(0.99)
df['mileage'] = np.where(df['mileage'] > mileage_99th_percentile, mileage_99th_percentile, df['mileage'])
print(f"Mileage capped at 99th percentile: {mileage_99th_percentile:,.0f} miles")

# Select final features for the model
features = ['make', 'fuel', 'transmission', 'body', 'drivetrain', 'cylinders', 'doors', 'mileage', 'car_age']
target = 'price'

numerical_features = ['cylinders', 'doors', 'mileage', 'car_age']
categorical_features = ['make', 'fuel', 'transmission', 'body', 'drivetrain']

# Create transformers for different column types
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')), # Fill missing numerical data with median
    ('scaler', StandardScaler())                    # Scale numerical data
])

# Filter out high cardinality/noisy features like 'name', 'description', 'model', 'trim', 'exterior_color', 'interior_color', 'engine'
# And use a reasonable subset of categorical features
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')), # Fill missing categorical data with mode
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # One-Hot Encode
])

# Create a ColumnTransformer to apply different preprocessing to different features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='drop' # Drop all other columns not specified
)

model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, max_depth=15, min_samples_leaf=5))
])

# Prepare data for training
X = df[features]
y = df[target]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("\nStarting model training...")
# Train the model
model.fit(X_train, y_train)
print("Model training complete.")

y_pred = model.predict(X_test)

# Evaluate metrics
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\n--- Model Evaluation Results ---")
print(f"Mean Absolute Error (MAE): ${mae:,.2f} (Average prediction error)")
print(f"R-squared Score (R2): {r2:.4f} (Variance explained)")

mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred)) # Root Mean Squared Error added

print("\n--- Model Evaluation Results ---")
print(f"Mean Absolute Error (MAE): ${mae:,.2f} (Average prediction error)")
print(f"Root Mean Squared Error (RMSE): ${rmse:,.2f} (Penalizes larger errors)")
print(f"R-squared Score (R2): {r2:.4f} (Variance explained)")

try:
    # Get feature names after one-hot encoding
    feature_names = list(model.named_steps['preprocessor'].named_transformers_['num'].get_feature_names_out(numerical_features))

    # Get the feature names from the OneHotEncoder step
    ohe_features = list(model.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features))

    final_features = feature_names + ohe_features

    # Get importance scores
    importances = model.named_steps['regressor'].feature_importances_

    # Create a Series for sorting
    feature_importances = pd.Series(importances, index=final_features).sort_values(ascending=False).head(15)

    plt.figure(figsize=(12, 6))
    sns.barplot(x=feature_importances.values, y=feature_importances.index, palette="viridis")
    plt.title('Top 15 Feature Importances in Random Forest Model')
    plt.xlabel('Importance Score')
    plt.ylabel('Feature')
    plt.show()

except Exception as e:
    print(f"Could not display feature importance: {e}")

def predict_vehicle_price(new_data):
    """
    Takes a dictionary of vehicle specs and predicts the price.
    new_data = {
        'make': 'Toyota',
        'fuel': 'Gasoline',
        'transmission': 'Automatic',
        'body': 'SUV',
        'drivetrain': 'All-wheel Drive',
        'cylinders': 6,
        'doors': 4,
        'mileage': 30000,
        'car_age': 5
    }
    """
    # Create a DataFrame from the new data
    new_df = pd.DataFrame([new_data])

    # Ensure all required features are present, fill with 'unknown' or median if missing
    for col in features:
        if col not in new_df.columns:
            if col in numerical_features:
                new_df[col] = X_train[col].median() # Use median from training data
            else:
                new_df[col] = 'unknown' # Use a sentinel value for categorical data

    # Make prediction
    predicted_price = model.predict(new_df[features])[0]
    return f"${predicted_price:,.2f}"

# Example Prediction
example_specs = {
    'make': 'Ford',
    'fuel': 'Gasoline',
    'transmission': 'Automatic',
    'body': 'Sedan',
    'drivetrain': 'Front-wheel Drive',
    'cylinders': 4.0,
    'doors': 4.0,
    'mileage': 15000,
    'car_age': 2
}

print("\n--- Example Prediction ---")
print(f"Specs: {example_specs}")
predicted_price = predict_vehicle_price(example_specs)
print(f"Predicted Price: {predicted_price}")

